{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "np.random.seed(77)\n",
    "tf.set_random_seed(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "train_data_path = \"./../data/train_data.csv\"\n",
    "test_data_path = \"./../data/test_data.csv\"\n",
    "target_tf_graph_path = \"./../trained_model/tf_graph/\"\n",
    "target_tf_ckpt_path = \"./../trained_model/tf_ckpt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data set\n",
    "train_data = pd.read_csv(train_data_path, header=None)\n",
    "test_data = pd.read_csv(test_data_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (60000, 784)\n",
      "Train Labels: (60000, 10)\n",
      "Test Data: (6000, 784)\n",
      "Test Labels: (6000,)\n"
     ]
    }
   ],
   "source": [
    "# separating features and labels\n",
    "train_X = train_data.iloc[:, :784]\n",
    "train_Y = pd.get_dummies(train_data[784])\n",
    "test_X = test_data.iloc[:, :784]\n",
    "test_Y = np.array(test_data[784])\n",
    "print(\"Train Data: \" + str(train_X.shape))\n",
    "print(\"Train Labels: \" + str(train_Y.shape))\n",
    "print(\"Test Data: \" + str(test_X.shape))\n",
    "print(\"Test Labels: \" + str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input], name=\"x\")\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name=\"y\")\n",
    "\n",
    "# first layer\n",
    "weight1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='weight1')\n",
    "bias1 = tf.Variable(tf.random_normal([n_hidden_1]), name=\"bias1\")\n",
    "layer_1 = tf.add(tf.matmul(x, weight1), bias1, name=\"layer_1\")\n",
    "relu_layer_1 = tf.nn.relu(layer_1, name=\"relu_layer_1\")\n",
    "\n",
    "# second layer\n",
    "weight2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name=\"weight2\")\n",
    "bias2 = tf.Variable(tf.random_normal([n_hidden_2]), name=\"bias2\")\n",
    "layer_2 = tf.add(tf.matmul(relu_layer_1, weight2), bias2, name=\"layer_2\")\n",
    "relu_layer_2 = tf.nn.relu(layer_2, name=\"relu_layer_2\")\n",
    "\n",
    "# output layer\n",
    "weight3 = tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name=\"weight3\")\n",
    "bias3 = tf.Variable(tf.random_normal([n_classes]), name=\"bias3\")\n",
    "out_layer = tf.add(tf.matmul(relu_layer_2, weight3), bias3, name=\"out_layer\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=y), name=\"cost\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"optimizer\").minimize(cost)\n",
    "\n",
    "# prediction\n",
    "pred = tf.argmax(out_layer, 1, name=\"pred\")\t#DON'T DELETE -> , output_type=tf.int32)\n",
    "\n",
    "# accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(test_Y, pred), \"float\"), name=\"accuracy\")\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')\n",
    "\n",
    "# making tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialzing session\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 489.854188958\n",
      "Epoch: 1 Loss: 111.257320518\n",
      "Epoch: 2 Loss: 70.3497079235\n",
      "Epoch: 3 Loss: 49.4658918114\n",
      "Epoch: 4 Loss: 36.1068981728\n",
      "Epoch: 5 Loss: 26.6590237814\n",
      "Epoch: 6 Loss: 19.9439513914\n",
      "Epoch: 7 Loss: 15.0354471329\n",
      "Epoch: 8 Loss: 11.3817763374\n",
      "Epoch: 9 Loss: 8.8470573209\n",
      "Epoch: 10 Loss: 6.76135195826\n",
      "Epoch: 11 Loss: 5.15355000727\n",
      "Epoch: 12 Loss: 3.94492233171\n",
      "Epoch: 13 Loss: 3.14803752235\n",
      "Epoch: 14 Loss: 2.59821283588\n"
     ]
    }
   ],
   "source": [
    "# training network\n",
    "num_batches = int(math.ceil(len(train_X) / float(batch_size)))\n",
    "for e in range(training_epochs):\n",
    "\tavg_cost = 0\n",
    "\tbatch_index = 0\n",
    "\twhile(batch_index < num_batches):\n",
    "\t\t# making train batches\n",
    "\t\tbatch_X = train_X.iloc[batch_index * batch_size : (batch_index + 1) * batch_size, :]\n",
    "\t\tbatch_Y = train_Y.iloc[batch_index * batch_size : (batch_index + 1) * batch_size, :]\n",
    "\t\tbatch_index += 1\n",
    "\t\t# training network\n",
    "\t\t_, loss = sess.run([optimizer, cost], feed_dict={x: batch_X, y: batch_Y})\n",
    "\t\tavg_cost += loss\n",
    "\tavg_cost = avg_cost / float(len(train_X))\n",
    "\tprint(\"Epoch: \" + str(e) + \" Loss: \" + str(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.982166666667   0.982167\n"
     ]
    }
   ],
   "source": [
    "# testing network\n",
    "predictions = sess.run(pred, feed_dict={x: test_X})\n",
    "\n",
    "# getting accuracy\n",
    "acc = accuracy_score(test_Y, predictions)\n",
    "tf_acc = sess.run(accuracy, feed_dict={x: test_X})\n",
    "\n",
    "print(\"Test Accuracy: \" + str(acc) + \"   \" + str(tf_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../trained_model/tf_ckpt/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving all trainable variables (weights and bias)\n",
    "for variable in tf.trainable_variables():\n",
    "\ttensor = tf.constant(variable.eval(sess))\n",
    "\ttf.assign(variable, tensor, name='nWeights')\n",
    "tf.train.write_graph(sess.graph_def, target_tf_graph_path, 'graph.pb', as_text=False)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, target_tf_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
